{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Automatica: Improved Implementation\n",
    "# Combined analysis and preprocessing with interactive feature selection\n",
    "\n",
    "# %%\n",
    "import gradio as gr\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygwalker as pyg\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Import additional required libraries at the top of your file\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple  # Added Tuple here\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jinja2\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Base Classes\n",
    "\n",
    "# %%\n",
    "class TorchWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Wrapper class to make PyTorch models compatible with scikit-learn\"\"\"\n",
    "    def __init__(self, model_class, model_params=None, criterion=nn.MSELoss, \n",
    "                 lr=0.001, batch_size=32, epochs=100, device=None):\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params or {}\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "        Required for scikit-learn compatibility.\"\"\"\n",
    "        params = {\n",
    "            'model_class': self.model_class,\n",
    "            'model_params': self.model_params,\n",
    "            'criterion': self.criterion,\n",
    "            'lr': self.lr,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'device': self.device\n",
    "        }\n",
    "        if deep:\n",
    "            params['model_params'] = copy.deepcopy(self.model_params)\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "        Required for scikit-learn compatibility.\"\"\"\n",
    "        for key, value in params.items():\n",
    "            if key == 'model_params':\n",
    "                self.model_params.update(value)\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize model\n",
    "        self.model = self.model_class(**self.model_params).to(self.device)\n",
    "        \n",
    "        # Convert data to tensors\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        y = torch.FloatTensor(y.reshape(-1, 1)).to(self.device)  # Ensure y is 2D\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset = TensorDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = self.criterion()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(batch_X)\n",
    "                # Ensure output and batch_y have the same shape\n",
    "                output = output.view(-1, 1)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Basic prediction method for scikit-learn compatibility\"\"\"\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(X)\n",
    "            return output.view(-1).cpu().numpy()  # Ensure 1D output for scikit-learn\n",
    "\n",
    "@dataclass\n",
    "class ModelTrialResults:\n",
    "    \"\"\"Store results for a single model's trial run\"\"\"\n",
    "    model_name: str\n",
    "    train_losses: List[float]\n",
    "    val_losses: List[float]\n",
    "    train_metrics: Dict[str, float]\n",
    "    val_metrics: Dict[str, float]\n",
    "    training_time: float\n",
    "    peak_memory_usage: float\n",
    "\n",
    "class AutomaticaModelSelector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_type: str,\n",
    "        input_dim: int,\n",
    "        trial_epochs: int = 100,\n",
    "        batch_size: int = 128,\n",
    "        timeout_minutes: int = 10\n",
    "    ):\n",
    "        self.task_type = task_type\n",
    "        self.input_dim = input_dim  # This should be the dimension after preprocessing\n",
    "        self.trial_epochs = trial_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.timeout_minutes = timeout_minutes\n",
    "        self.results = {}\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def train_model(self, model, model_name, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Generic training function for both models\"\"\"\n",
    "        try:\n",
    "            # Convert data to proper format\n",
    "            X_train = X_train.astype(np.float32)\n",
    "            y_train = np.array(y_train).astype(np.float32)\n",
    "            X_val = X_val.astype(np.float32)\n",
    "            y_val = np.array(y_val).astype(np.float32)\n",
    "            \n",
    "            # Scale the target variable\n",
    "            y_scaler = StandardScaler()\n",
    "            y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "            y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).ravel()\n",
    "            \n",
    "            # Convert to tensors\n",
    "            X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
    "            y_train_tensor = torch.FloatTensor(y_train_scaled).to(self.device)\n",
    "            X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
    "            y_val_tensor = torch.FloatTensor(y_val_scaled).to(self.device)\n",
    "            \n",
    "            # Training setup remains the same...\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            model = model.to(self.device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Training tracking\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            start_time = time.time()\n",
    "            best_val_loss = float('inf')\n",
    "            patience = 3\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Training loop remains the same...\n",
    "            for epoch in range(self.trial_epochs):\n",
    "                if (time.time() - start_time) > (self.timeout_minutes * 60):\n",
    "                    print(f\"Training timeout after {epoch} epochs\")\n",
    "                    break\n",
    "                \n",
    "                # Training\n",
    "                model.train()\n",
    "                epoch_losses = []\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_losses.append(loss.item())\n",
    "                \n",
    "                avg_train_loss = np.mean(epoch_losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(X_val_tensor)\n",
    "                    val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "                    val_losses.append(val_loss)\n",
    "                \n",
    "                print(f\"{model_name} - Epoch {epoch+1}/{self.trial_epochs} - \"\n",
    "                    f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "            \n",
    "            # Calculate final metrics with scaled predictions\n",
    "            training_time = time.time() - start_time\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1024**2 if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # Calculate all metrics\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Get scaled predictions\n",
    "                train_preds_scaled = model(X_train_tensor).cpu().numpy()\n",
    "                val_preds_scaled = model(X_val_tensor).cpu().numpy()\n",
    "                \n",
    "                # Calculate metrics on scaled data first\n",
    "                train_metrics = {\n",
    "                    'MSE': mean_squared_error(y_train_scaled, train_preds_scaled),\n",
    "                    'MAE': np.mean(np.abs(y_train_scaled - train_preds_scaled)),\n",
    "                    'R2': r2_score(y_train_scaled, train_preds_scaled)\n",
    "                }\n",
    "                \n",
    "                val_metrics = {\n",
    "                    'MSE': mean_squared_error(y_val_scaled, val_preds_scaled),\n",
    "                    'MAE': np.mean(np.abs(y_val_scaled - val_preds_scaled)),\n",
    "                    'R2': r2_score(y_val_scaled, val_preds_scaled)\n",
    "                }\n",
    "            \n",
    "            # Store results\n",
    "            self.results[model_name] = ModelTrialResults(\n",
    "                model_name=model_name,\n",
    "                train_losses=train_losses,\n",
    "                val_losses=val_losses,\n",
    "                train_metrics=train_metrics,\n",
    "                val_metrics=val_metrics,\n",
    "                training_time=training_time,\n",
    "                peak_memory_usage=peak_memory\n",
    "            )\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Detailed error in {model_name} training: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def _calculate_metrics(self, model, X, y) -> Dict[str, float]:\n",
    "        \"\"\"Calculate task-specific metrics\"\"\"\n",
    "        metrics = {}\n",
    "        y_pred = model(torch.FloatTensor(X).to(self.device)).cpu().detach().numpy()\n",
    "        \n",
    "        if self.task_type == 'regression':\n",
    "            metrics['MSE'] = mean_squared_error(y, y_pred)\n",
    "            metrics['MAE'] = np.mean(np.abs(y - y_pred))\n",
    "            metrics['RMSE'] = math.sqrt(mean_squared_error(y, y_pred))\n",
    "            metrics['R2'] = r2_score(y, y_pred)\n",
    "        else:\n",
    "            metrics['Accuracy'] = (y_pred == y).mean()\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def run_trial(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Run trials for all models\"\"\"\n",
    "        print(f\"Input dimension after preprocessing: {X_train.shape[1]}\")\n",
    "        \n",
    "        # Train ResNetMLP\n",
    "        resnet = ResNetMLP(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dims=(256, 512, 256),\n",
    "            output_dim=1 if self.task_type == 'regression' else len(np.unique(y_train))\n",
    "        )\n",
    "        success_resnet = self.train_model(resnet, 'ResNetMLP', X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Train TransformerTabular\n",
    "        transformer = TransformerTabular(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=1 if self.task_type == 'regression' else len(np.unique(y_train))\n",
    "        )\n",
    "        success_transformer = self.train_model(transformer, 'TransformerTabular', X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Train DenseNetTabular\n",
    "        densenet = DenseNetTabular(\n",
    "            input_dim=X_train.shape[1],\n",
    "            growth_rate=32,\n",
    "            block_config=(6, 12, 24, 16),\n",
    "            output_dim=1 if self.task_type == 'regression' else len(np.unique(y_train))\n",
    "        )\n",
    "        success_densenet = self.train_model(densenet, 'DenseNetTabular', X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Train TabNetModel\n",
    "        tabnet = TabNetModel(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=1 if self.task_type == 'regression' else len(np.unique(y_train))\n",
    "        )\n",
    "        success_tabnet = self.train_model(tabnet, 'TabNetModel', X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Print validation metrics for all models\n",
    "        for model_name, results in self.results.items():\n",
    "            val_metrics = results.val_metrics\n",
    "            print(f\"\\n{model_name} Validation Metrics:\")\n",
    "            print(f\"MSE: {val_metrics['MSE']:.4f}\")\n",
    "            print(f\"MAE: {val_metrics['MAE']:.4f}\")\n",
    "            print(f\"R2: {val_metrics['R2']:.4f}\")\n",
    "        \n",
    "        return any([success_resnet, success_transformer, success_densenet, success_tabnet])\n",
    "\n",
    "    def plot_training_curves(self) -> go.Figure:\n",
    "        \"\"\"Plot comparative training curves\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=('Training Loss', 'Validation Loss'),\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.15\n",
    "        )\n",
    "        \n",
    "        colors = {\n",
    "            'ResNetMLP': 'blue',\n",
    "            'TransformerTabular': 'red',\n",
    "            'DenseNetTabular': 'green',\n",
    "            'TabNetModel': 'purple'\n",
    "        }\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            # Training loss\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(range(1, len(results.train_losses) + 1)),\n",
    "                    y=results.train_losses,\n",
    "                    name=f'{model_name} (Train)',\n",
    "                    line=dict(color=colors.get(model_name, 'gray')),\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Validation loss\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(range(1, len(results.val_losses) + 1)),\n",
    "                    y=results.val_losses,\n",
    "                    name=f'{model_name} (Val)',\n",
    "                    line=dict(color=colors.get(model_name, 'gray'), dash='dash'),\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=600,\n",
    "            title_text=\"Model Comparison - Training Progress\",\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Epoch\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def plot_metrics_comparison(self) -> go.Figure:\n",
    "        \"\"\"Plot comparative metrics including MSE, MAE, and R2\"\"\"\n",
    "        metrics_data = []\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            # Get validation metrics\n",
    "            val_metrics = results.val_metrics\n",
    "            \n",
    "            # Add each metric as a separate row for MSE, MAE, and R2\n",
    "            metrics_data.extend([\n",
    "                {\n",
    "                    'Model': model_name,\n",
    "                    'Metric': 'MSE',\n",
    "                    'Value': val_metrics['MSE']\n",
    "                },\n",
    "                {\n",
    "                    'Model': model_name,\n",
    "                    'Metric': 'MAE',\n",
    "                    'Value': val_metrics['MAE']\n",
    "                },\n",
    "                {\n",
    "                    'Model': model_name,\n",
    "                    'Metric': 'R2',\n",
    "                    'Value': val_metrics['R2']\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_metrics = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # Create the comparison plot\n",
    "        fig = px.bar(\n",
    "            df_metrics,\n",
    "            x='Model',\n",
    "            y='Value',\n",
    "            color='Metric',\n",
    "            title='Model Comparison - Validation Metrics',\n",
    "            barmode='group',\n",
    "            template=\"plotly_white\",\n",
    "            color_discrete_map={\n",
    "                'MSE': 'rgb(147, 155, 255)',  # Light blue\n",
    "                'MAE': 'rgb(255, 127, 127)',  # Light red\n",
    "                'R2': 'rgb(102, 204, 153)'    # Light green\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Update layout for better readability\n",
    "        fig.update_layout(\n",
    "            height=500,\n",
    "            xaxis_title=\"Model Type\",\n",
    "            yaxis_title=\"Metric Value\",\n",
    "            legend_title=\"Metric\",\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                gridcolor='rgba(0,0,0,0.1)',\n",
    "                showgrid=True\n",
    "            ),\n",
    "            plot_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        # Add specific colors and adjust order if needed\n",
    "        fig.update_traces(\n",
    "            dict(\n",
    "                marker_line_width=0.5,\n",
    "                marker_line_color='black',\n",
    "                opacity=0.8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "    def plot_resource_usage(self) -> go.Figure:\n",
    "        \"\"\"Plot comparative resource usage\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Training Time (s)', 'Peak Memory Usage (MB)')\n",
    "        )\n",
    "        \n",
    "        model_names = list(self.results.keys())\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=model_names,\n",
    "                y=[results.training_time for results in self.results.values()],\n",
    "                name='Training Time'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=model_names,\n",
    "                y=[results.peak_memory_usage for results in self.results.values()],\n",
    "                name='Memory Usage'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=400,\n",
    "            title_text=\"Model Comparison - Resource Usage\",\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Model Architectures\n",
    "\n",
    "# %%\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_input_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(num_input_features, growth_rate),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.layer(x)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class DenseNetTabular(nn.Module):\n",
    "    \"\"\"DenseNet-inspired architecture for tabular data with dense connections\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        growth_rate: int = 32,\n",
    "        block_config: Tuple[int, ...] = (6, 12, 24, 16),\n",
    "        output_dim: int = 1,\n",
    "        dropout_rate: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial layer\n",
    "        self.features = []\n",
    "        num_features = growth_rate * 2\n",
    "        \n",
    "        # Initial dense layer\n",
    "        self.features.append(nn.Sequential(\n",
    "            nn.Linear(input_dim, num_features),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.GELU()\n",
    "        ))\n",
    "        \n",
    "        # Dense blocks\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = self._make_dense_block(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            self.features.append(block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            # Transition layers\n",
    "            if i != len(block_config) - 1:\n",
    "                self.features.append(self._make_transition(\n",
    "                    num_input_features=num_features,\n",
    "                    num_output_features=num_features // 2\n",
    "                ))\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        self.features = nn.Sequential(*self.features)\n",
    "        \n",
    "        # Final classification/regression layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features // 2),\n",
    "            nn.BatchNorm1d(num_features // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def _make_dense_block(self, num_layers, num_input_features, growth_rate, dropout_rate):\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate,\n",
    "                dropout_rate\n",
    "            ))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_transition(self, num_input_features, num_output_features):\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm1d(num_input_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(num_input_features, num_output_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.classifier(features)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "class TabNetModel(nn.Module):\n",
    "    \"\"\"TabNet implementation with feature selection and self-attention\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int = 1,\n",
    "        feature_dim: int = 64,\n",
    "        output_dim_per_step: int = 8,\n",
    "        num_steps: int = 3,\n",
    "        num_shared: int = 2,\n",
    "        attention_dim: int = 32,\n",
    "        dropout_rate: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # Feature transformer\n",
    "        self.feature_transform = nn.Sequential(\n",
    "            nn.Linear(input_dim, feature_dim),\n",
    "            nn.BatchNorm1d(feature_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feature_dim, feature_dim),\n",
    "                nn.BatchNorm1d(feature_dim),\n",
    "                nn.GELU()\n",
    "            ) for _ in range(num_shared)\n",
    "        ])\n",
    "        \n",
    "        # Step-specific layers\n",
    "        self.step_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feature_dim, output_dim_per_step),\n",
    "                nn.BatchNorm1d(output_dim_per_step),\n",
    "                nn.GELU()\n",
    "            ) for _ in range(num_steps)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_transform = nn.Sequential(\n",
    "            nn.Linear(output_dim_per_step * num_steps, output_dim_per_step * num_steps // 2),\n",
    "            nn.BatchNorm1d(output_dim_per_step * num_steps // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(output_dim_per_step * num_steps // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature transformation\n",
    "        x = self.feature_transform(x)\n",
    "        \n",
    "        # Store step outputs\n",
    "        step_outputs = []\n",
    "        prior_scales = torch.ones_like(x)\n",
    "        \n",
    "        # Process steps\n",
    "        for step in range(self.num_steps):\n",
    "            # Apply shared layers\n",
    "            h = x\n",
    "            for shared in self.shared_layers:\n",
    "                h = shared(h)\n",
    "            \n",
    "            # Compute attention masks\n",
    "            mask = self.attention(h)\n",
    "            masked_x = x * mask * prior_scales\n",
    "            \n",
    "            # Update prior scales\n",
    "            prior_scales = prior_scales * (1 - mask)\n",
    "            \n",
    "            # Apply step-specific transformation\n",
    "            step_out = self.step_layers[step](masked_x)\n",
    "            step_outputs.append(step_out)\n",
    "        \n",
    "        # Combine step outputs\n",
    "        combined = torch.cat(step_outputs, dim=1)\n",
    "        \n",
    "        # Final output transformation\n",
    "        out = self.output_transform(combined)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "class TransformerTabular(nn.Module):\n",
    "    \"\"\"Transformer architecture adapted for tabular data\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        output_dim: int = 1,\n",
    "        dropout_rate: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Modified positional embedding for batched input\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layers with reduced complexity\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add feature dimension for transformer\n",
    "        x = x.unsqueeze(1)  # Shape: [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Input embedding\n",
    "        x = self.input_embedding(x)  # Shape: [batch_size, 1, hidden_dim]\n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Take the output of the first (and only) position\n",
    "        x = x.squeeze(1)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "class ResNetMLP(nn.Module):\n",
    "    \"\"\"Deep Residual MLP with skip connections for tabular data\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: tuple = (256, 512, 256),\n",
    "        output_dim: int = 1,\n",
    "        dropout_rate: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build main layers with residual connections\n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            # Add residual connection if dimensions match\n",
    "            if prev_dim == dim:\n",
    "                layers.append(nn.Linear(dim, dim))\n",
    "                layers.append(lambda x: x + layers[-2](x))\n",
    "                \n",
    "            prev_dim = dim\n",
    "            \n",
    "        # Output layer\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Training System\n",
    "\n",
    "# %%\n",
    "class AutomaticaTrainer:\n",
    "    \"\"\"Handles model training and hyperparameter optimization\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: str,\n",
    "        input_dim: int,\n",
    "        task_type: str,\n",
    "        output_dim: int = 1,\n",
    "        n_iter: int = 25,\n",
    "        cv: int = 3,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        self.model_class = self._get_model_class(model_class)\n",
    "        self.input_dim = input_dim\n",
    "        self.task_type = task_type\n",
    "        self.output_dim = output_dim\n",
    "        self.n_iter = n_iter\n",
    "        self.cv = cv\n",
    "        self.random_state = random_state\n",
    "        self.best_params = None\n",
    "        self.best_model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = StandardScaler()\n",
    "\n",
    "    def _get_model_class(self, model_name):\n",
    "        \"\"\"Get the model class from the name\"\"\"\n",
    "        model_map = {\n",
    "            'ResNetMLP': ResNetMLP,\n",
    "            'TransformerTabular': TransformerTabular,\n",
    "            'DenseNetTabular': DenseNetTabular,\n",
    "            'TabNetModel': TabNetModel\n",
    "        }\n",
    "        return model_map[model_name]\n",
    "\n",
    "    def _get_param_space(self):\n",
    "        \"\"\"Define hyperparameter search space based on model\"\"\"\n",
    "        param_space = {\n",
    "            'model_params__input_dim': [self.input_dim],\n",
    "            'model_params__output_dim': [self.output_dim],\n",
    "            'lr': Real(1e-4, 1e-2, prior='log-uniform'),\n",
    "            'batch_size': Categorical([32, 64, 128, 256]),\n",
    "            'epochs': Integer(50, 200)\n",
    "        }\n",
    "        \n",
    "        # Model-specific parameters\n",
    "        if self.model_class == DenseNetTabular:\n",
    "            param_space.update({\n",
    "                'model_params__growth_rate': Integer(16, 64),\n",
    "                'model_params__dropout_rate': Real(0.1, 0.5)\n",
    "            })\n",
    "        elif self.model_class == TabNetModel:\n",
    "            param_space.update({\n",
    "                'model_params__feature_dim': Categorical([32, 64]),\n",
    "                'model_params__dropout_rate': Real(0.1, 0.5)\n",
    "            })\n",
    "        elif self.model_class == TransformerTabular:\n",
    "            param_space.update({\n",
    "                'model_params__hidden_dim': Categorical([128, 256]),\n",
    "                'model_params__num_heads': Categorical([4, 8]),\n",
    "                'model_params__dropout_rate': Real(0.1, 0.5)\n",
    "            })\n",
    "            \n",
    "        return param_space\n",
    "\n",
    "    def optimize_hyperparameters(self, X_train, y_train):\n",
    "        \"\"\"Run Bayesian optimization for hyperparameter tuning\"\"\"\n",
    "        print(\"Starting hyperparameter optimization...\")\n",
    "        \n",
    "        # Scale the data\n",
    "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        \n",
    "        # Create base model wrapper\n",
    "        base_model = TorchWrapper(\n",
    "            model_class=self.model_class,\n",
    "            model_params={'input_dim': self.input_dim, 'output_dim': self.output_dim}\n",
    "        )\n",
    "        \n",
    "        # Configure search\n",
    "        param_space = self._get_param_space()\n",
    "        optimizer = BayesSearchCV(\n",
    "            base_model,\n",
    "            param_space,\n",
    "            n_iter=self.n_iter,\n",
    "            cv=self.cv,\n",
    "            scoring='neg_mean_squared_error' if self.task_type == 'regression' else 'accuracy',\n",
    "            n_jobs=1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Run optimization\n",
    "        optimizer.fit(X_train_scaled, y_train_scaled)\n",
    "        \n",
    "        self.best_params = optimizer.best_params_\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def train_final_model(self, X_train, y_train, X_val, y_val, patience=5, min_delta=1e-4):\n",
    "        \"\"\"Train final model with early stopping and improved scaling\"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"Run optimize_hyperparameters first\")\n",
    "            \n",
    "        # Initialize model with best parameters\n",
    "        model_params = {k.replace('model_params__', ''): v \n",
    "                    for k, v in self.best_params.items() \n",
    "                    if k.startswith('model_params__')}\n",
    "        \n",
    "        self.best_model = self.model_class(**model_params).to(self.device)\n",
    "        \n",
    "        # Scale the data\n",
    "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.feature_scaler.transform(X_val)\n",
    "        \n",
    "        if self.task_type == 'regression':\n",
    "            y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "            y_val_scaled = self.target_scaler.transform(y_val.reshape(-1, 1)).ravel()\n",
    "        else:\n",
    "            y_train_scaled = y_train\n",
    "            y_val_scaled = y_val\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train = torch.FloatTensor(X_train_scaled).to(self.device)\n",
    "        y_train = torch.FloatTensor(y_train_scaled).to(self.device)\n",
    "        X_val = torch.FloatTensor(X_val_scaled).to(self.device)\n",
    "        y_val = torch.FloatTensor(y_val_scaled).to(self.device)\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.best_params['batch_size'],\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Training setup with improved stability\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.best_model.parameters(),\n",
    "            lr=self.best_params['lr'],\n",
    "            weight_decay=0.01  # L2 regularization\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Training tracking\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_metric': [],\n",
    "            'val_metric': []\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        pbar = tqdm(range(self.best_params['epochs']), desc=\"Training Progress\")\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            # Training\n",
    "            self.best_model.train()\n",
    "            train_losses = []\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.best_model(batch_X)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.best_model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "            \n",
    "            # Validation\n",
    "            self.best_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = self.best_model(X_val)\n",
    "                val_loss = criterion(val_output, y_val).item()\n",
    "            \n",
    "            # Store metrics\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # Calculate scaled metrics\n",
    "            train_pred = self.best_model(X_train).detach().cpu().numpy()\n",
    "            val_pred = val_output.cpu().numpy()\n",
    "            \n",
    "            if self.task_type == 'regression':\n",
    "                # Convert predictions back to original scale for metrics\n",
    "                train_pred_orig = self.target_scaler.inverse_transform(train_pred.reshape(-1, 1)).ravel()\n",
    "                val_pred_orig = self.target_scaler.inverse_transform(val_pred.reshape(-1, 1)).ravel()\n",
    "                \n",
    "                train_metric = r2_score(y_train.cpu().numpy(), train_pred)\n",
    "                val_metric = r2_score(y_val.cpu().numpy(), val_pred)\n",
    "            else:\n",
    "                train_metric = accuracy_score(y_train.cpu().numpy(), train_pred > 0.5)\n",
    "                val_metric = accuracy_score(y_val.cpu().numpy(), val_pred > 0.5)\n",
    "            \n",
    "            history['train_metric'].append(train_metric)\n",
    "            history['val_metric'].append(val_metric)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': f'{avg_train_loss:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'val_metric': f'{val_metric:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def plot_optimization_results(self):\n",
    "        \"\"\"Plot the optimization results\"\"\"\n",
    "        if not hasattr(self, 'optimizer') or not hasattr(self.optimizer, 'cv_results_'):\n",
    "            return go.Figure()\n",
    "            \n",
    "        results = self.optimizer.cv_results_\n",
    "        \n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(results['mean_test_score']))),\n",
    "                y=-np.array(results['mean_test_score']),\n",
    "                name=\"Mean Test Score\",\n",
    "                mode='lines+markers'\n",
    "            ),\n",
    "            secondary_y=False\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(results['std_test_score']))),\n",
    "                y=results['std_test_score'],\n",
    "                name=\"Score Std Dev\",\n",
    "                mode='lines+markers'\n",
    "            ),\n",
    "            secondary_y=True\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title_text=\"Hyperparameter Optimization Progress\",\n",
    "            showlegend=True,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Iteration\")\n",
    "        fig.update_yaxes(title_text=\"Mean Test Score\", secondary_y=False)\n",
    "        fig.update_yaxes(title_text=\"Score Standard Deviation\", secondary_y=True)\n",
    "        \n",
    "        self.optimization_plot = fig\n",
    "        return fig\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot the training history\"\"\"\n",
    "        if not history:\n",
    "            return go.Figure()\n",
    "            \n",
    "        fig = make_subplots(rows=2, cols=1,\n",
    "                            subplot_titles=('Loss', 'Metrics'))\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(history['train_loss']))),\n",
    "                y=history['train_loss'],\n",
    "                name=\"Train Loss\",\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(history['val_loss']))),\n",
    "                y=history['val_loss'],\n",
    "                name=\"Val Loss\",\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(history['train_metric']))),\n",
    "                y=history['train_metric'],\n",
    "                name=\"Train Metric\",\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(history['val_metric']))),\n",
    "                y=history['val_metric'],\n",
    "                name=\"Val Metric\",\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=600,\n",
    "            title_text=\"Training History\",\n",
    "            showlegend=True,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        self.training_plot = fig\n",
    "        return fig\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"No trained model available\")\n",
    "            \n",
    "        save_dict = {\n",
    "            'model_state': self.best_model.state_dict(),\n",
    "            'model_class': self.model_class.__name__,\n",
    "            'params': self.best_params,\n",
    "            'task_type': self.task_type,\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'feature_scaler': self.feature_scaler,\n",
    "            'target_scaler': self.target_scaler,\n",
    "            'optimization_plot': self.optimization_plot,\n",
    "            'training_plot': self.training_plot\n",
    "        }\n",
    "        \n",
    "        dump(save_dict, filepath)  # Using imported joblib.dump\n",
    "        print(f\"Model and scalers saved to {filepath}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML template for the report\n",
    "REPORT_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Automatica ML Analysis Report</title>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        body { \n",
    "            padding: 20px;\n",
    "            font-family: system-ui, -apple-system, \"Segoe UI\", Roboto, sans-serif;\n",
    "        }\n",
    "        .container { max-width: 1200px; }\n",
    "        .section { margin-bottom: 40px; }\n",
    "        .plot-container { margin: 20px 0; }\n",
    "        .metric-card {\n",
    "            background: #f8f9fa;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        pre {\n",
    "            background: #f8f9fa;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "        }\n",
    "        .params-table {\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .card-body {\n",
    "            text-align: center;\n",
    "            padding: 15px;\n",
    "        }\n",
    "        .display-6 {\n",
    "            font-size: 1.5rem;\n",
    "            font-weight: 500;\n",
    "        }\n",
    "        .features-container {\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            gap: 8px;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "\n",
    "        .feature-badge {\n",
    "            background-color: #e9ecef;\n",
    "            padding: 6px 12px;\n",
    "            border-radius: 16px;\n",
    "            font-size: 0.9rem;\n",
    "            color: #495057;\n",
    "        }\n",
    "        .model-architecture {\n",
    "            font-family: 'Consolas', 'Monaco', monospace;\n",
    "            background: #f8f9fa;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            line-height: 1.5;\n",
    "            overflow-x: auto;\n",
    "            color: #212529;\n",
    "        }\n",
    "        .table {\n",
    "            width: 100%;\n",
    "            margin-bottom: 1rem;\n",
    "            background-color: transparent;\n",
    "            border-collapse: collapse;\n",
    "        }\n",
    "\n",
    "        .table thead th {\n",
    "            vertical-align: bottom;\n",
    "            border-bottom: 2px solid #dee2e6;\n",
    "            background-color: #f8f9fa;\n",
    "            padding: 12px;\n",
    "            text-align: left;\n",
    "        }\n",
    "\n",
    "        .table tbody td {\n",
    "            padding: 12px;\n",
    "            border-top: 1px solid #dee2e6;\n",
    "        }\n",
    "\n",
    "        .table-hover tbody tr:hover {\n",
    "            background-color: rgba(0, 0, 0, 0.02);\n",
    "        }\n",
    "\n",
    "        .table-responsive {\n",
    "            display: block;\n",
    "            width: 100%;\n",
    "            overflow-x: auto;\n",
    "            -webkit-overflow-scrolling: touch;\n",
    "            margin-bottom: 1rem;\n",
    "        }\n",
    "\n",
    "        .mt-4 {\n",
    "            margin-top: 1.5rem;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1 class=\"mb-4\">Automatica ML Analysis Report</h1>\n",
    "        <p class=\"text-muted\">Generated on {{ datetime.now().strftime('%B %d, %Y %H:%M:%S') }}</p>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Executive Summary</h2>\n",
    "            <div class=\"metric-card\">\n",
    "                <p>This report provides a comprehensive analysis of the deep learning pipeline executed using Automatica.</p>\n",
    "                <ul>\n",
    "                    <li><strong>Dataset:</strong> {{ dataset_name }}</li>\n",
    "                    <li><strong>Target Variable:</strong> {{ target_variable }}</li>\n",
    "                    <li><strong>Task Type:</strong> {{ task_type }}</li>\n",
    "                    <li><strong>Features:</strong> {{ n_features }}</li>\n",
    "                    <li><strong>Total Samples:</strong> {{ n_samples }}</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Data Preprocessing</h2>\n",
    "            <h3>Data Split</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <ul>\n",
    "                    <li>Training Set: {{ preprocessing.train_size }} samples</li>\n",
    "                    <li>Validation Set: {{ preprocessing.val_size }} samples</li>\n",
    "                    <li>Test Set: {{ preprocessing.test_size }} samples</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "\n",
    "            <h3>Target Distribution</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ plots.target_distribution | safe }}\n",
    "            </div>\n",
    "\n",
    "            <h3>Feature Distributions</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ plots.feature_distributions | safe }}\n",
    "            </div>\n",
    "\n",
    "            <h3>Feature Relationships</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ plots.feature_relationships | safe }}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Model Selection</h2>\n",
    "            \n",
    "            <h3>Training Progress</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ plots.training_curves | safe }}\n",
    "            </div>\n",
    "\n",
    "            <h3>Model Comparison</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ plots.metrics_comparison | safe }}\n",
    "            </div>\n",
    "\n",
    "            <h3>Resource Usage</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ plots.resource_usage | safe }}\n",
    "            </div>\n",
    "\n",
    "            <h3>Model Selection Summary</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <h4>Training Metrics</h4>\n",
    "                <div class=\"table-responsive\">\n",
    "                    <table class=\"table table-hover\">\n",
    "                        <thead>\n",
    "                            <tr>\n",
    "                                <th>Model</th>\n",
    "                                <th>Training Loss</th>\n",
    "                                <th>Validation Loss</th>\n",
    "                                <th>Training Time</th>\n",
    "                                <th>Memory Usage</th>\n",
    "                            </tr>\n",
    "                        </thead>\n",
    "                        <tbody>\n",
    "                            {% for model_name, results in model_selection.results.items() %}\n",
    "                            <tr>\n",
    "                                <td><strong>{{ model_name }}</strong></td>\n",
    "                                <td>{{ \"%.4f\"|format(results.train_losses[-1]) }}</td>\n",
    "                                <td>{{ \"%.4f\"|format(results.val_losses[-1]) }}</td>\n",
    "                                <td>{{ \"%.2f\"|format(results.training_time) }}s</td>\n",
    "                                <td>{{ \"%.1f\"|format(results.peak_memory_usage) }} MB</td>\n",
    "                            </tr>\n",
    "                            {% endfor %}\n",
    "                        </tbody>\n",
    "                    </table>\n",
    "                </div>\n",
    "\n",
    "                <h4 class=\"mt-4\">Performance Metrics</h4>\n",
    "                <div class=\"table-responsive\">\n",
    "                    <table class=\"table table-hover\">\n",
    "                        <thead>\n",
    "                            <tr>\n",
    "                                <th>Model</th>\n",
    "                                <th>MSE</th>\n",
    "                                <th>MAE</th>\n",
    "                                <th>R²</th>\n",
    "                            </tr>\n",
    "                        </thead>\n",
    "                        <tbody>\n",
    "                            {% for model_name, results in model_selection.results.items() %}\n",
    "                            <tr>\n",
    "                                <td><strong>{{ model_name }}</strong></td>\n",
    "                                <td>{{ \"%.4f\"|format(results.val_metrics.MSE) }}</td>\n",
    "                                <td>{{ \"%.4f\"|format(results.val_metrics.MAE) }}</td>\n",
    "                                <td>{{ \"%.4f\"|format(results.val_metrics.R2) }}</td>\n",
    "                            </tr>\n",
    "                            {% endfor %}\n",
    "                        </tbody>\n",
    "                    </table>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Final Model Training</h2>\n",
    "            \n",
    "            <h3>Selected Model Details</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <h4>Model: {{ final_training.selected_model }}</h4>\n",
    "                <h4>Best Hyperparameters:</h4>\n",
    "                <div class=\"params-table\">\n",
    "                    <table class=\"table table-striped\">\n",
    "                        <thead>\n",
    "                            <tr>\n",
    "                                <th>Parameter</th>\n",
    "                                <th>Value</th>\n",
    "                            </tr>\n",
    "                        </thead>\n",
    "                        <tbody>\n",
    "                            {% for key, value in final_training.best_params.items() %}\n",
    "                            <tr>\n",
    "                                <td>{{ key }}</td>\n",
    "                                <td>{{ value }}</td>\n",
    "                            </tr>\n",
    "                            {% endfor %}\n",
    "                        </tbody>\n",
    "                    </table>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <h3>Training Progress</h3>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ final_training.optimization_plot | safe }}\n",
    "            </div>\n",
    "            <div class=\"plot-container\">\n",
    "                {{ final_training.training_plot | safe }}\n",
    "            </div>\n",
    "\n",
    "            <h3>Performance Metrics</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"row\">\n",
    "                    {% for key, value in final_training.final_metrics.items() %}\n",
    "                    <div class=\"col-md-6 mb-3\">\n",
    "                        <div class=\"card\">\n",
    "                            <div class=\"card-body\">\n",
    "                                <h5 class=\"card-title\">{{ key }}</h5>\n",
    "                                <p class=\"card-text display-6\">{{ \"%.4f\"|format(value) }}</p>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    {% endfor %}\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Technical Details</h2>\n",
    "            \n",
    "            <h3>Feature Engineering</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <h4>Selected Features:</h4>\n",
    "                <div class=\"features-container\">\n",
    "                    {% for feature in technical_details.selected_features %}\n",
    "                        <span class=\"feature-badge\">{{ feature }}</span>\n",
    "                    {% endfor %}\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <h3>Model Architecture</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"model-architecture\">\n",
    "                    {{ technical_details.model_architecture | replace(\"  \", \"&nbsp;&nbsp;\") | replace(\"\\n\", \"<br>\") | safe }}\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <h3>Training Configuration</h3>\n",
    "            <div class=\"metric-card\">\n",
    "                <ul>\n",
    "                    <li>Batch Size: {{ technical_details.training_config.batch_size }}</li>\n",
    "                    <li>Learning Rate: {{ technical_details.training_config.learning_rate }}</li>\n",
    "                    <li>Optimizer: {{ technical_details.training_config.optimizer }}</li>\n",
    "                    <li>Loss Function: {{ technical_details.training_config.loss_function }}</li>\n",
    "                    <li>Early Stopping Patience: {{ technical_details.training_config.early_stopping_patience }}</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "class AutomaticaReporter:\n",
    "    \"\"\"Handles report generation for Automatica ML pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.report_data = {\n",
    "            \"dataset_info\": {},\n",
    "            \"preprocessing\": {},\n",
    "            \"model_selection\": {},\n",
    "            \"final_training\": {},\n",
    "            \"plots\": {},\n",
    "            \"metrics\": {},\n",
    "            \"technical_details\": {}\n",
    "        }\n",
    "        \n",
    "    def gather_data(self, ui_instance):\n",
    "        \"\"\"Gather all necessary data from UI instance and its components\"\"\"\n",
    "        try:\n",
    "            # Dataset Information\n",
    "            self.gather_dataset_info(ui_instance)\n",
    "            \n",
    "            # Preprocessing Information\n",
    "            self.gather_preprocessing_info(ui_instance)\n",
    "            \n",
    "            # Model Selection Information\n",
    "            self.gather_model_selection_info(ui_instance)\n",
    "            \n",
    "            # Final Training Information\n",
    "            self.gather_training_info(ui_instance)\n",
    "            \n",
    "            # Technical Details\n",
    "            self.gather_technical_details(ui_instance)\n",
    "            \n",
    "            return True, \"Data gathered successfully\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error gathering report data: {str(e)}\"\n",
    "    \n",
    "    def gather_dataset_info(self, ui):\n",
    "        \"\"\"Gather dataset information\"\"\"\n",
    "        try:\n",
    "            if hasattr(ui, 'preprocessor') and ui.preprocessor is not None:\n",
    "                # Get dataset name\n",
    "                dataset_name = \"Uploaded Dataset\"\n",
    "                if hasattr(ui, 'file_input') and ui.file_input is not None:\n",
    "                    dataset_name = os.path.basename(ui.file_input.name)\n",
    "                \n",
    "        #         # Get other info\n",
    "        #         self.report_data[\"dataset_info\"].update({\n",
    "        #             \"dataset_name\": dataset_name,\n",
    "        #             \"target_variable\": ui.target_var if ui.target_var is not None else \"Unknown\",\n",
    "        #             \"task_type\": ui.model_type if ui.model_type is not None else \"Unknown\",\n",
    "        #             \"n_features\": ui.X_train.shape[1] if hasattr(ui, 'X_train') else 0,\n",
    "        #             \"n_samples\": len(ui.X_train) if hasattr(ui, 'X_train') else 0,\n",
    "        #             \"feature_names\": ui.feature_names if hasattr(ui, 'feature_names') else []\n",
    "        #         })\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error gathering dataset info: {str(e)}\")\n",
    "        #     # Set default values if there's an error\n",
    "        #     self.report_data[\"dataset_info\"].update({\n",
    "        #         \"dataset_name\": \"Unknown Dataset\",\n",
    "        #         \"target_variable\": \"Unknown\",\n",
    "        #         \"task_type\": \"Unknown\",\n",
    "        #         \"n_features\": 0,\n",
    "        #         \"n_samples\": 0,\n",
    "        #         \"feature_names\": []\n",
    "        #     })\n",
    "        \n",
    "                    # Calculate total features and samples from original data\n",
    "            n_features = len(ui.important_features) if hasattr(ui, 'important_features') else 0\n",
    "            \n",
    "            # Calculate total samples by summing all splits\n",
    "            total_samples = (\n",
    "                (len(ui.X_train) if hasattr(ui, 'X_train') else 0) +\n",
    "                (len(ui.X_val) if hasattr(ui, 'X_val') else 0) +\n",
    "                (len(ui.X_test) if hasattr(ui, 'X_test') else 0)\n",
    "            )\n",
    "            \n",
    "            # Get other info\n",
    "            self.report_data[\"dataset_info\"].update({\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"target_variable\": ui.target_var if ui.target_var is not None else \"Unknown\",\n",
    "                \"task_type\": ui.model_type if ui.model_type is not None else \"Unknown\",\n",
    "                \"n_features\": n_features,\n",
    "                \"n_samples\": total_samples,\n",
    "                \"feature_names\": ui.feature_names if hasattr(ui, 'feature_names') else []\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error gathering dataset info: {str(e)}\")\n",
    "            # Set default values if there's an error\n",
    "            self.report_data[\"dataset_info\"].update({\n",
    "                \"dataset_name\": \"Unknown Dataset\",\n",
    "                \"target_variable\": \"Unknown\",\n",
    "                \"task_type\": \"Unknown\",\n",
    "                \"n_features\": 0,\n",
    "                \"n_samples\": 0,\n",
    "                \"feature_names\": []\n",
    "            })\n",
    "    \n",
    "    def gather_preprocessing_info(self, ui):\n",
    "        \"\"\"Gather preprocessing information\"\"\"\n",
    "        if hasattr(ui, 'X_train') and ui.X_train is not None:\n",
    "            self.report_data[\"preprocessing\"].update({\n",
    "                \"train_size\": ui.X_train.shape[0],\n",
    "                \"val_size\": ui.X_val.shape[0] if hasattr(ui, 'X_val') else 0,\n",
    "                \"test_size\": ui.X_test.shape[0] if hasattr(ui, 'X_test') else 0\n",
    "            })\n",
    "            \n",
    "            # Store preprocessing plots with error handling for each plot\n",
    "            try:\n",
    "                if hasattr(ui, 'target_dist_plot') and ui.target_dist_plot is not None:\n",
    "                    self.report_data[\"plots\"][\"target_distribution\"] = ui.target_dist_plot\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing target distribution plot: {str(e)}\")\n",
    "                \n",
    "            try:\n",
    "                if hasattr(ui, 'feature_dist_plot') and ui.feature_dist_plot is not None:\n",
    "                    self.report_data[\"plots\"][\"feature_distributions\"] = ui.feature_dist_plot\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing feature distributions plot: {str(e)}\")\n",
    "                \n",
    "            try:\n",
    "                if hasattr(ui, 'feature_rels_plot') and ui.feature_rels_plot is not None:\n",
    "                    self.report_data[\"plots\"][\"feature_relationships\"] = ui.feature_rels_plot\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing feature relationships plot: {str(e)}\")\n",
    "    \n",
    "    def gather_model_selection_info(self, ui):\n",
    "        \"\"\"Gather model selection information\"\"\"\n",
    "        if hasattr(ui, 'model_selector') and ui.model_selector is not None:\n",
    "            model_selector = ui.model_selector\n",
    "            \n",
    "            # Get plots from model selection\n",
    "            self.report_data[\"plots\"].update({\n",
    "                \"training_curves\": model_selector.plot_training_curves(),\n",
    "                \"metrics_comparison\": model_selector.plot_metrics_comparison(),\n",
    "                \"resource_usage\": model_selector.plot_resource_usage()\n",
    "            })\n",
    "            \n",
    "            # Store model selection results\n",
    "            self.report_data[\"model_selection\"][\"results\"] = {\n",
    "                model_name: {\n",
    "                    \"train_losses\": results.train_losses,\n",
    "                    \"val_losses\": results.val_losses,\n",
    "                    \"train_metrics\": results.train_metrics,\n",
    "                    \"val_metrics\": results.val_metrics,\n",
    "                    \"training_time\": results.training_time,\n",
    "                    \"peak_memory_usage\": results.peak_memory_usage\n",
    "                }\n",
    "                for model_name, results in model_selector.results.items()\n",
    "            }\n",
    "    \n",
    "    def gather_training_info(self, ui):\n",
    "        \"\"\"Gather final model training information\"\"\"\n",
    "        if hasattr(ui, 'selected_model_name'):\n",
    "            self.report_data[\"final_training\"].update({\n",
    "                \"selected_model\": ui.selected_model_name,\n",
    "                \"best_params\": ui.best_params if hasattr(ui, 'best_params') else {},\n",
    "                \"optimization_plot\": ui.optimization_plot if hasattr(ui, 'optimization_plot') else None,\n",
    "                \"training_plot\": ui.training_plot if hasattr(ui, 'training_plot') else None,\n",
    "                \"final_metrics\": ui.final_metrics if hasattr(ui, 'final_metrics') else {}\n",
    "            })\n",
    "    \n",
    "    def gather_technical_details(self, ui):\n",
    "        \"\"\"Gather technical implementation details\"\"\"\n",
    "        self.report_data[\"technical_details\"].update({\n",
    "            \"selected_features\": ui.important_features if hasattr(ui, 'important_features') else [],\n",
    "            \"model_architecture\": str(ui.best_model) if hasattr(ui, 'best_model') else \"Not available\",\n",
    "            \"training_config\": {\n",
    "                \"batch_size\": ui.batch_size if hasattr(ui, 'batch_size') else \"Unknown\",\n",
    "                \"learning_rate\": ui.learning_rate if hasattr(ui, 'learning_rate') else \"Unknown\",\n",
    "                \"optimizer\": \"AdamW\",  # Currently hardcoded in the implementation\n",
    "                \"loss_function\": \"MSELoss\",  # Currently hardcoded for regression\n",
    "                \"early_stopping_patience\": 5  # Currently hardcoded\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    def _convert_plot_to_html(self, plot):\n",
    "        \"\"\"Convert a plotly figure to a self-contained HTML div\"\"\"\n",
    "        if plot is None:\n",
    "            return \"\"\n",
    "        try:\n",
    "            if isinstance(plot, (dict, str)):  # If it's already converted somehow\n",
    "                return str(plot)\n",
    "            \n",
    "            # Ensure we're working with a plotly figure\n",
    "            if not isinstance(plot, go.Figure):\n",
    "                print(f\"Warning: plot is of type {type(plot)}, expected plotly.graph_objects.Figure\")\n",
    "                return \"\"\n",
    "            \n",
    "            # Convert to HTML with CDN-hosted plotly.js\n",
    "            html_str = plot.to_html(\n",
    "                full_html=False,\n",
    "                include_plotlyjs='cdn',\n",
    "                config={'displayModeBar': True}\n",
    "            )\n",
    "            \n",
    "            return html_str\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting plot to HTML: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def generate_report(self, output_path=\"automatica_report.html\"):\n",
    "        try:\n",
    "            from datetime import datetime\n",
    "            import pprint\n",
    "            \n",
    "            # Convert all plots to HTML\n",
    "            plot_html = {}\n",
    "            for plot_name, plot in self.report_data[\"plots\"].items():\n",
    "                plot_html[plot_name] = self._convert_plot_to_html(plot)\n",
    "            \n",
    "            # Convert plots to HTML before template rendering\n",
    "            if self.report_data[\"final_training\"].get(\"optimization_plot\"):\n",
    "                self.report_data[\"final_training\"][\"optimization_plot\"] = self._convert_plot_to_html(\n",
    "                    self.report_data[\"final_training\"][\"optimization_plot\"]\n",
    "                )\n",
    "                \n",
    "            if self.report_data[\"final_training\"].get(\"training_plot\"):\n",
    "                self.report_data[\"final_training\"][\"training_plot\"] = self._convert_plot_to_html(\n",
    "                    self.report_data[\"final_training\"][\"training_plot\"]\n",
    "                )\n",
    "\n",
    "            # Create template from string\n",
    "            template = Template(REPORT_TEMPLATE)\n",
    "            \n",
    "            # Combine all data for the template\n",
    "            template_data = {\n",
    "                **self.report_data[\"dataset_info\"],\n",
    "                \"preprocessing\": self.report_data[\"preprocessing\"],\n",
    "                # \"plots\": self.report_data[\"plots\"],\n",
    "                \"plots\": plot_html,\n",
    "                \"model_selection\": self.report_data[\"model_selection\"],\n",
    "                \"final_training\": self.report_data[\"final_training\"],\n",
    "                \"technical_details\": self.report_data[\"technical_details\"],\n",
    "                \"datetime\": datetime,\n",
    "                \"pprint\": pprint.pformat\n",
    "            }\n",
    "\n",
    "            # Render template\n",
    "            html_content = template.render(**template_data)\n",
    "            \n",
    "            # Write to file\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            \n",
    "            return True, f\"Report generated successfully at {output_path}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error generating report: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## UI System\n",
    "\n",
    "# %%\n",
    "class AutomaticaUI:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = None\n",
    "        self.feature_names = None\n",
    "        self.important_features = None\n",
    "        self.model_selector = None\n",
    "        self.selected_model_name = None\n",
    "        self.X_train_state = None\n",
    "        self.y_train_state = None\n",
    "        self.X_val_state = None\n",
    "        self.y_val_state = None\n",
    "        self.model_selection_results = None\n",
    "        self.reporter = AutomaticaReporter()\n",
    "        self.target_var = None\n",
    "        self.model_type = None\n",
    "        self.file_input = None\n",
    "        self.batch_size = None\n",
    "        self.learning_rate = None\n",
    "        self.best_params = None\n",
    "        self.best_model = None\n",
    "        self.final_metrics = None\n",
    "        self.selected_model_name = None\n",
    "        self.optimization_plot = None\n",
    "        self.training_plot = None\n",
    "    \n",
    "    def load_and_validate_file(self, file):\n",
    "        \"\"\"Validate and load CSV file\"\"\"\n",
    "        try:\n",
    "            if file is None:\n",
    "                self.file_input = None  # Clear the stored file info\n",
    "                return None, \"No file uploaded\", gr.update(choices=[]), None\n",
    "            \n",
    "            file_extension = os.path.splitext(file.name)[1].lower()\n",
    "            if file_extension != '.csv':\n",
    "                self.file_input = None  # Clear the stored file info\n",
    "                return None, \"Only CSV files are supported\", gr.update(choices=[]), None\n",
    "            \n",
    "            df = pd.read_csv(file.name)\n",
    "            self.file_input = file  # Store the file info\n",
    "            \n",
    "            return (\n",
    "                \"File loaded successfully\", \n",
    "                self.create_pygwalker_vis(df),\n",
    "                gr.update(choices=self.get_column_names(df)),\n",
    "                df\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.file_input = None  # Clear the stored file info\n",
    "            return None, f\"Error loading file: {str(e)}\", gr.update(choices=[]), None\n",
    "    \n",
    "    def create_pygwalker_vis(self, df):\n",
    "        \"\"\"Create PyGWalker visualization\"\"\"\n",
    "        if df is None:\n",
    "            return \"No data to visualize\"\n",
    "        try:\n",
    "            gw = pyg.walk(df)\n",
    "            return gw.to_html()\n",
    "        except Exception as e:\n",
    "            return f\"Error creating visualization: {str(e)}\"\n",
    "    \n",
    "    def get_column_names(self, df):\n",
    "        \"\"\"Get column names for target selection\"\"\"\n",
    "        if df is None:\n",
    "            return []\n",
    "        return list(df.columns)\n",
    "    \n",
    "    def update_feature_selector(self, df, target, model_type):\n",
    "        \"\"\"Update feature selector dropdown based on target and model type\"\"\"\n",
    "        if df is not None and target:\n",
    "            important_features = self.get_important_features(df, target, model_type)\n",
    "            return gr.update(choices=important_features, value=important_features[:5])\n",
    "        return gr.update(choices=[], value=[])\n",
    "\n",
    "    def get_important_features(self, df, target_variable, model_type, n_features=10):\n",
    "        \"\"\"Identify important features using mutual information\"\"\"\n",
    "        try:\n",
    "            X = df.drop(columns=[target_variable])\n",
    "            y = df[target_variable]\n",
    "            \n",
    "            # Handle categorical features\n",
    "            categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "            X_encoded = X.copy()\n",
    "            for col in categorical_cols:\n",
    "                X_encoded[col] = pd.Categorical(X_encoded[col]).codes\n",
    "            \n",
    "            # Calculate mutual information scores\n",
    "            mi_func = mutual_info_regression if model_type == \"Regression\" else mutual_info_classif\n",
    "            mi_scores = mi_func(X_encoded, y)\n",
    "            \n",
    "            # Create feature importance DataFrame\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': mi_scores\n",
    "            })\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            \n",
    "            return feature_importance['feature'].tolist()[:n_features]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature importance calculation: {str(e)}\")\n",
    "            return X.columns.tolist()[:n_features]\n",
    "        \n",
    "    def analyze_and_preprocess(self, df, target_variable, model_type, selected_features):\n",
    "        \"\"\"Combined analysis and preprocessing function\"\"\"\n",
    "        if df is None or target_variable is None:\n",
    "            return None, None, None, \"Please provide data and target variable\"\n",
    "        \n",
    "        try:\n",
    "            # 1. Preprocess data first to get transformed features\n",
    "            X = df[selected_features]\n",
    "            y = df[target_variable]\n",
    "            \n",
    "            # Create preprocessing pipeline\n",
    "            numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "            categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "            \n",
    "            numerical_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler() if model_type == 'Regression' \n",
    "                        else MinMaxScaler())\n",
    "            ])\n",
    "            \n",
    "            categorical_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', \n",
    "                                    sparse_output=False))\n",
    "            ])\n",
    "            \n",
    "            self.preprocessor = ColumnTransformer([\n",
    "                ('num', numerical_pipeline, numerical_cols),\n",
    "                ('cat', categorical_pipeline, categorical_cols)\n",
    "            ])\n",
    "            \n",
    "            # Transform all data for visualization\n",
    "            X_preprocessed = self.preprocessor.fit_transform(X)\n",
    "            \n",
    "            # Get feature names after preprocessing\n",
    "            if len(categorical_cols) > 0:\n",
    "                encoded_cat_cols = self.preprocessor.named_transformers_[\n",
    "                    'cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "                self.feature_names = list(numerical_cols) + list(encoded_cat_cols)\n",
    "            else:\n",
    "                self.feature_names = list(numerical_cols)\n",
    "            \n",
    "            # Create DataFrame with preprocessed data\n",
    "            X_preprocessed_df = pd.DataFrame(\n",
    "                X_preprocessed, \n",
    "                columns=self.feature_names,\n",
    "                index=X.index\n",
    "            )\n",
    "            \n",
    "            # Split the data\n",
    "            X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42,\n",
    "                stratify=y if model_type == 'Classification' else None\n",
    "            )\n",
    "            \n",
    "            self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "                X_temp, y_temp, test_size=0.25, random_state=42,\n",
    "                stratify=y_temp if model_type == 'Classification' else None\n",
    "            )\n",
    "            \n",
    "            # Transform split data\n",
    "            self.X_train = self.preprocessor.transform(self.X_train)\n",
    "            self.X_val = self.preprocessor.transform(self.X_val)\n",
    "            self.X_test = self.preprocessor.transform(self.X_test)\n",
    "            \n",
    "            status = (f\"Data preprocessing completed successfully!\\n\"\n",
    "                    f\"Training set size: {self.X_train.shape}\\n\"\n",
    "                    f\"Validation set size: {self.X_val.shape}\\n\"\n",
    "                    f\"Test set size: {self.X_test.shape}\")\n",
    "            \n",
    "            # Store selected features for later use\n",
    "            self.important_features = selected_features\n",
    "            \n",
    "            # Create visualizations\n",
    "            target_dist = self.create_target_distribution(df, target_variable, model_type)\n",
    "            feature_dist = self.create_feature_distributions(X, X_preprocessed_df, selected_features, numerical_cols)\n",
    "            feature_rels = self.create_feature_relationships(df, X, target_variable, selected_features, numerical_cols, model_type)\n",
    "            \n",
    "            # Create visualizations and store them as instance attributes\n",
    "            self.target_dist_plot = self.create_target_distribution(df, target_variable, model_type)\n",
    "            self.feature_dist_plot = self.create_feature_distributions(X, X_preprocessed_df, selected_features, numerical_cols)\n",
    "            self.feature_rels_plot = self.create_feature_relationships(df, X, target_variable, selected_features, numerical_cols, model_type)\n",
    "            \n",
    "            return (\n",
    "                target_dist,\n",
    "                feature_dist, \n",
    "                feature_rels, \n",
    "                status,\n",
    "                self.X_train,\n",
    "                self.y_train,\n",
    "                self.X_val,\n",
    "                self.y_val\n",
    "            )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Detailed error: {str(e)}\")\n",
    "            return None, None, None, f\"Error in analysis and preprocessing: {str(e)}\"\n",
    "    \n",
    "    def create_target_distribution(self, df, target_variable, model_type):\n",
    "        \"\"\"Create target distribution visualization\"\"\"\n",
    "        if model_type == \"Regression\":\n",
    "            return px.histogram(\n",
    "                df, \n",
    "                x=target_variable,\n",
    "                title=f\"Distribution of {target_variable}\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        else:\n",
    "            value_counts = df[target_variable].value_counts().reset_index()\n",
    "            value_counts.columns = ['Class', 'Count']\n",
    "            return px.bar(\n",
    "                value_counts,\n",
    "                x='Class',\n",
    "                y='Count',\n",
    "                title=f\"Class Distribution of {target_variable}\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "    \n",
    "    def create_feature_distributions(self, X, X_preprocessed_df, selected_features, numerical_cols):\n",
    "        \"\"\"Create feature distribution visualizations\"\"\"\n",
    "        feature_dist = make_subplots(\n",
    "            rows=len(selected_features), \n",
    "            cols=2,\n",
    "            subplot_titles=[\n",
    "                f\"{feat} - Before Preprocessing\" if i % 2 == 0 \n",
    "                else f\"{feat} - After Preprocessing\"\n",
    "                for feat in selected_features\n",
    "                for i in range(2)\n",
    "            ],\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, feature in enumerate(selected_features, 1):\n",
    "            # Original distribution\n",
    "            if feature in numerical_cols:\n",
    "                # Numerical features - use histogram\n",
    "                feature_dist.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=X[feature],\n",
    "                        name=f\"Original {feature}\",\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "            else:\n",
    "                # Categorical features - use bar chart\n",
    "                value_counts = X[feature].value_counts()\n",
    "                feature_dist.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=value_counts.index,\n",
    "                        y=value_counts.values,\n",
    "                        name=f\"Original {feature}\",\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "            \n",
    "            # Preprocessed distribution\n",
    "            preprocessed_cols = [col for col in X_preprocessed_df.columns if feature in col]\n",
    "            if len(preprocessed_cols) == 1:\n",
    "                # Single preprocessed column (numerical)\n",
    "                feature_dist.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=X_preprocessed_df[preprocessed_cols[0]],\n",
    "                        name=f\"Preprocessed {feature}\",\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=i, col=2\n",
    "                )\n",
    "            else:\n",
    "                # Multiple preprocessed columns (one-hot encoded)\n",
    "                feature_dist.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=preprocessed_cols,\n",
    "                        y=[X_preprocessed_df[col].sum() for col in preprocessed_cols],\n",
    "                        name=f\"Preprocessed {feature}\",\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=i, col=2\n",
    "                )\n",
    "        \n",
    "        feature_dist.update_layout(\n",
    "            height=300 * len(selected_features),\n",
    "            title_text=\"Feature Distributions Before and After Preprocessing\",\n",
    "            showlegend=False,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        return feature_dist\n",
    "\n",
    "    def create_feature_relationships(self, df, X, target_variable, selected_features, numerical_cols, model_type):\n",
    "        \"\"\"Create feature relationship visualizations\"\"\"\n",
    "        feature_rels = make_subplots(\n",
    "            rows=len(selected_features), \n",
    "            cols=1,\n",
    "            subplot_titles=[f\"{target_variable} vs {feat}\" for feat in selected_features],\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, feature in enumerate(selected_features, 1):\n",
    "            if feature in numerical_cols:\n",
    "                # Scatter plot for numerical features\n",
    "                feature_rels.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=X[feature],\n",
    "                        y=df[target_variable],\n",
    "                        mode='markers',\n",
    "                        name=feature,\n",
    "                        marker=dict(\n",
    "                            size=6,\n",
    "                            opacity=0.6,\n",
    "                            colorscale='Viridis'\n",
    "                        ),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "            else:\n",
    "                # Box plot for categorical features\n",
    "                feature_rels.add_trace(\n",
    "                    go.Box(\n",
    "                        x=X[feature],\n",
    "                        y=df[target_variable],\n",
    "                        name=feature,\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "            \n",
    "            # Update axes labels\n",
    "            feature_rels.update_xaxes(title_text=feature, row=i, col=1)\n",
    "            if i == len(selected_features):  # Only add y-axis title for the last subplot\n",
    "                feature_rels.update_yaxes(title_text=target_variable, row=i, col=1)\n",
    "        \n",
    "        feature_rels.update_layout(\n",
    "            height=300 * len(selected_features),\n",
    "            title_text=f\"Feature Relationships with {target_variable}\",\n",
    "            showlegend=False,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        return feature_rels\n",
    "\n",
    "    def handle_model_training(self, df_state, target_var, selected_model, X_train, y_train, X_val, y_val, task_type):\n",
    "        \"\"\"Handle model training with hyperparameter optimization\"\"\"\n",
    "        try:\n",
    "            if X_train is None or y_train is None:\n",
    "                return None, None, \"Error: Please run data preprocessing first\"\n",
    "\n",
    "            # Data preparation\n",
    "            X_train = np.array(X_train) if not isinstance(X_train, np.ndarray) else X_train\n",
    "            y_train = np.array(y_train) if not isinstance(y_train, np.ndarray) else y_train\n",
    "            X_val = np.array(X_val) if not isinstance(X_val, np.ndarray) else X_val\n",
    "            y_val = np.array(y_val) if not isinstance(y_val, np.ndarray) else y_val\n",
    "            \n",
    "            print(f\"Training data shapes - X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "            \n",
    "            # Store selected model name\n",
    "            self.selected_model_name = selected_model\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = AutomaticaTrainer(\n",
    "                model_class=selected_model,\n",
    "                input_dim=X_train.shape[1],\n",
    "                task_type=task_type.lower(),\n",
    "                output_dim=1 if task_type.lower() == 'regression' else len(np.unique(y_train)),\n",
    "                n_iter=5\n",
    "            )\n",
    "            \n",
    "            # Run hyperparameter optimization\n",
    "            optimizer = trainer.optimize_hyperparameters(X_train, y_train.reshape(-1, 1))\n",
    "            \n",
    "            # Generate and store optimization plot\n",
    "            self.optimization_plot = trainer.plot_optimization_results()\n",
    "            \n",
    "            # Store best parameters\n",
    "            self.best_params = trainer.best_params\n",
    "            \n",
    "            # Train final model\n",
    "            training_history = trainer.train_final_model(\n",
    "                X_train, \n",
    "                y_train.reshape(-1, 1),\n",
    "                X_val,\n",
    "                y_val.reshape(-1, 1),\n",
    "                patience=5,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            # Generate and store training plot\n",
    "            self.training_plot = trainer.plot_training_history(training_history)\n",
    "        \n",
    "            # Store model and configuration\n",
    "            self.best_model = trainer.best_model\n",
    "            self.batch_size = trainer.best_params.get('batch_size', 32)\n",
    "            self.learning_rate = trainer.best_params.get('lr', 0.001)\n",
    "            \n",
    "            # Save model\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = f'best_model_{timestamp}.joblib'\n",
    "            trainer.save_model(model_filename)\n",
    "            \n",
    "            # Generate summary\n",
    "            summary = f\"\"\"\n",
    "    Training completed successfully!\n",
    "\n",
    "    Model: {selected_model}\n",
    "    Best Parameters: {trainer.best_params}\n",
    "    Model saved as: {model_filename}\n",
    "\n",
    "    Final Metrics:\n",
    "    Train Loss: {training_history['train_loss'][-1]:.4f}\n",
    "    Validation Loss: {training_history['val_loss'][-1]:.4f}\n",
    "    Train Metric: {training_history['train_metric'][-1]:.4f}\n",
    "    Validation Metric: {training_history['val_metric'][-1]:.4f}\n",
    "    \"\"\"\n",
    "            \n",
    "            # Store metrics\n",
    "            self.final_metrics = self._parse_metrics_from_summary(summary)\n",
    "            \n",
    "            # Return plots and summary\n",
    "            return self.optimization_plot, self.training_plot, summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Detailed error in model training: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            return None, None, f\"Error in model training: {str(e)}\"\n",
    "\n",
    "    def _parse_metrics_from_summary(self, summary):\n",
    "        \"\"\"Helper method to parse metrics from training summary\"\"\"\n",
    "        metrics = {}\n",
    "        try:\n",
    "            # Basic parsing of the summary string to extract metrics\n",
    "            lines = summary.split('\\n')\n",
    "            for line in lines:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    try:\n",
    "                        metrics[key.strip()] = float(value.strip())\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metrics: {str(e)}\")\n",
    "        return metrics\n",
    "    \n",
    "    def run_model_selection(self, df_state, target_var, model_type, X_train_state, y_train_state, X_val_state, y_val_state):\n",
    "        \"\"\"Run model selection process and return results\"\"\"\n",
    "        try:\n",
    "            # Get data from state variables\n",
    "            if X_train_state is None or y_train_state is None:\n",
    "                return None, None, None, \"Please run data preprocessing first\", []\n",
    "\n",
    "            # Convert data if needed\n",
    "            X_train = np.array(X_train_state) if not isinstance(X_train_state, np.ndarray) else X_train_state\n",
    "            y_train = np.array(y_train_state) if not isinstance(y_train_state, np.ndarray) else y_train_state\n",
    "            X_val = np.array(X_val_state) if not isinstance(X_val_state, np.ndarray) else X_val_state\n",
    "            y_val = np.array(y_val_state) if not isinstance(y_val_state, np.ndarray) else y_val_state\n",
    "                \n",
    "            print(f\"X_train shape: {X_train.shape}\")  # Debug print\n",
    "            \n",
    "            # Initialize selector with actual input dimension from preprocessed data\n",
    "            self.model_selector = AutomaticaModelSelector(\n",
    "                task_type=model_type.lower(),\n",
    "                input_dim=X_train.shape[1],\n",
    "                trial_epochs=100,\n",
    "                batch_size=128\n",
    "            )\n",
    "            \n",
    "            # Run trials with preprocessed data\n",
    "            success = self.model_selector.run_trial(\n",
    "                X_train, \n",
    "                y_train, \n",
    "                X_val, \n",
    "                y_val\n",
    "            )\n",
    "            \n",
    "            if not success:\n",
    "                return None, None, None, \"Model training failed\", []\n",
    "            \n",
    "            # Get plots\n",
    "            training_curves = self.model_selector.plot_training_curves()\n",
    "            metrics_plot = self.model_selector.plot_metrics_comparison()\n",
    "            resource_plot = self.model_selector.plot_resource_usage()\n",
    "            \n",
    "            # Get summary as string\n",
    "            summary_data = []\n",
    "            for model_name, results in self.model_selector.results.items():\n",
    "                summary_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Final Train Loss': f\"{results.train_losses[-1]:.4f}\",\n",
    "                    'Final Val Loss': f\"{results.val_losses[-1]:.4f}\",\n",
    "                    'Training Time (s)': f\"{results.training_time:.2f}\",\n",
    "                    'Memory (MB)': f\"{results.peak_memory_usage:.1f}\"\n",
    "                })\n",
    "                summary_data[-1].update({\n",
    "                    k: f\"{v:.4f}\" for k, v in results.val_metrics.items()\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_text = \"Model Comparison Summary:\\n\" + summary_df.to_string()\n",
    "            \n",
    "            # Store the results in the UI state\n",
    "            self.model_selection_results = {\n",
    "                'models': self.model_selector.results,\n",
    "                'best_model': model_name # Store the best model name based on validation metrics\n",
    "            }\n",
    "            \n",
    "            return (\n",
    "                training_curves,\n",
    "                metrics_plot,\n",
    "                resource_plot,\n",
    "                summary_text,\n",
    "                list(self.model_selector.results.keys())\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in model selection: {str(e)}\")\n",
    "            return None, None, None, f\"Error in model selection: {str(e)}\", []\n",
    "        \n",
    "    def handle_report_generation(self):\n",
    "        \"\"\"Handle the report generation process and open report in browser\"\"\"\n",
    "        try:\n",
    "            # Gather all data\n",
    "            success, message = self.reporter.gather_data(self)\n",
    "            if not success:\n",
    "                return f\"Failed to gather report data: {message}\"\n",
    "            \n",
    "            # Generate the report\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            report_path = f\"automatica_report_{timestamp}.html\"\n",
    "            success, message = self.reporter.generate_report(report_path)\n",
    "            \n",
    "            if success:\n",
    "                # Get the absolute path to the report\n",
    "                abs_path = os.path.abspath(report_path)\n",
    "                \n",
    "                # Convert the file path to a URL format\n",
    "                if os.name == 'nt':  # Windows\n",
    "                    url = f'file:///{abs_path.replace(os.sep, \"/\")}'\n",
    "                else:  # Unix-like systems\n",
    "                    url = f'file://{abs_path}'\n",
    "                \n",
    "                # Open the report in the default web browser\n",
    "                try:\n",
    "                    import webbrowser\n",
    "                    webbrowser.open(url)\n",
    "                    return f\"Report generated and opened in browser! Location: {report_path}\"\n",
    "                except Exception as browser_error:\n",
    "                    return f\"Report generated at {report_path}, but couldn't open browser: {str(browser_error)}\"\n",
    "            else:\n",
    "                return f\"Failed to generate report: {message}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error in report generation: {str(e)}\"\n",
    "\n",
    "    def build_interface(self):\n",
    "        with gr.Blocks() as demo:\n",
    "            # Create state objects\n",
    "            df_state = gr.State()\n",
    "            X_train_state = gr.State(None)\n",
    "            y_train_state = gr.State(None)\n",
    "            X_val_state = gr.State(None)\n",
    "            y_val_state = gr.State(None)\n",
    "            \n",
    "            gr.Markdown(\"# Automatica\")\n",
    "            \n",
    "            # File Upload\n",
    "            with gr.Row():\n",
    "                file_input = gr.File(label=\"Upload CSV File\")\n",
    "                file_status = gr.Textbox(label=\"File Status\", interactive=False)\n",
    "            \n",
    "            # Data Visualization\n",
    "            gr.Markdown(\"### Data Visualization\")\n",
    "            visualization_output = gr.HTML()\n",
    "            \n",
    "            # Model Configuration\n",
    "            gr.Markdown(\"### Model Configuration\")\n",
    "            with gr.Row():\n",
    "                target_var = gr.Dropdown(\n",
    "                    label=\"Target Variable\",\n",
    "                    choices=[],\n",
    "                    interactive=True\n",
    "                )\n",
    "                model_type = gr.Dropdown(\n",
    "                    label=\"Model Type\",\n",
    "                    choices=[\"Regression\", \"Classification\"],\n",
    "                    value=\"Regression\",\n",
    "                    interactive=True\n",
    "                )\n",
    "            \n",
    "            # Feature Selection\n",
    "            feature_selector = gr.Dropdown(\n",
    "                label=\"Select Features to Visualize\",\n",
    "                choices=[],\n",
    "                multiselect=True,\n",
    "                interactive=True\n",
    "            )\n",
    "            \n",
    "            # Tabs for different sections\n",
    "            with gr.Tabs():\n",
    "                # Data Processing Tab\n",
    "                with gr.TabItem(\"Data Processing\"):\n",
    "                    process_button = gr.Button(\"Analyze and Preprocess Data\")\n",
    "                    \n",
    "                    with gr.Tabs():\n",
    "                        with gr.TabItem(\"Target Distribution\"):\n",
    "                            target_dist_plot = gr.Plot()\n",
    "                        with gr.TabItem(\"Feature Distributions\"):\n",
    "                            feature_dist_plot = gr.Plot()\n",
    "                        with gr.TabItem(\"Feature Relationships\"):\n",
    "                            feature_rels_plot = gr.Plot()\n",
    "                    \n",
    "                    process_status = gr.Textbox(\n",
    "                        label=\"Processing Status\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "                \n",
    "                # Model Selection Tab\n",
    "                with gr.TabItem(\"Model Selection\"):\n",
    "                    gr.Markdown(\"### Model Selection\")\n",
    "                    select_model_button = gr.Button(\"Start Model Selection\")\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            training_curves_plot = gr.Plot(label=\"Training Curves\")\n",
    "                        with gr.Column():\n",
    "                            metrics_plot = gr.Plot(label=\"Model Metrics\")\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            resource_plot = gr.Plot(label=\"Resource Usage\")\n",
    "                        with gr.Column():\n",
    "                            model_summary = gr.Textbox(\n",
    "                                label=\"Model Selection Summary\",\n",
    "                                interactive=False,\n",
    "                                lines=10\n",
    "                            )\n",
    "                \n",
    "                # Model Training Tab\n",
    "                with gr.TabItem(\"Model Training\"):\n",
    "                    gr.Markdown(\"### Model Training and Hyperparameter Tuning\")\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        model_dropdown = gr.Dropdown(\n",
    "                            label=\"Select Model for Training\",\n",
    "                            choices=[\"ResNetMLP\", \"TransformerTabular\", \"DenseNetTabular\", \"TabNetModel\"],\n",
    "                            interactive=True\n",
    "                        )\n",
    "                        train_model_button = gr.Button(\"Start Training\")\n",
    "                    \n",
    "                    with gr.Tabs():\n",
    "                        with gr.TabItem(\"Optimization Progress\"):\n",
    "                            optimization_plot = gr.Plot(label=\"Hyperparameter Optimization\")\n",
    "                        with gr.TabItem(\"Training Progress\"):\n",
    "                            training_plot = gr.Plot(label=\"Training History\")\n",
    "                    \n",
    "                    training_summary = gr.Textbox(\n",
    "                        label=\"Training Summary\",\n",
    "                        interactive=False,\n",
    "                        lines=10\n",
    "                    )\n",
    "                    \n",
    "                    # Report Generation tab\n",
    "                with gr.TabItem(\"Report Generation\"):\n",
    "                    gr.Markdown(\"### Generate Analysis Report\")\n",
    "                    with gr.Row():\n",
    "                        generate_report_button = gr.Button(\"Generate Report\")\n",
    "                        report_status = gr.Textbox(\n",
    "                            label=\"Report Status\",\n",
    "                            interactive=False,\n",
    "                            lines=3\n",
    "                        )\n",
    "                    \n",
    "                    gr.Markdown(\"\"\"\n",
    "                    This will generate a comprehensive HTML report including:\n",
    "                    - Dataset analysis and preprocessing details\n",
    "                    - Model selection results and comparisons\n",
    "                    - Final model training results and performance metrics\n",
    "                    - Technical implementation details\n",
    "                    \"\"\")\n",
    "            \n",
    "            def handle_target_selection(target, model_type_value):\n",
    "                self.target_var = target\n",
    "                self.model_type = model_type_value\n",
    "                return None\n",
    "            \n",
    "            # Event handlers\n",
    "            file_input.change(\n",
    "                fn=self.load_and_validate_file,\n",
    "                inputs=[file_input],\n",
    "                outputs=[file_status, visualization_output, target_var, df_state]\n",
    "            )\n",
    "            \n",
    "            target_var.change(\n",
    "                fn=self.update_feature_selector,\n",
    "                inputs=[df_state, target_var, model_type],\n",
    "                outputs=[feature_selector]\n",
    "            ).success(\n",
    "                fn=lambda t, m: setattr(self, 'target_var', t) or setattr(self, 'model_type', m) or None,\n",
    "                inputs=[target_var, model_type],\n",
    "                outputs=None\n",
    "            )\n",
    "\n",
    "            model_type.change(\n",
    "                fn=lambda t, m: setattr(self, 'model_type', m) or None,\n",
    "                inputs=[target_var, model_type],\n",
    "                outputs=None\n",
    "            )\n",
    "            \n",
    "            process_button.click(\n",
    "                fn=self.analyze_and_preprocess,\n",
    "                inputs=[df_state, target_var, model_type, feature_selector],\n",
    "                outputs=[\n",
    "                    target_dist_plot,\n",
    "                    feature_dist_plot,\n",
    "                    feature_rels_plot,\n",
    "                    process_status,\n",
    "                    X_train_state,\n",
    "                    y_train_state,\n",
    "                    X_val_state,\n",
    "                    y_val_state\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            select_model_button.click(\n",
    "                fn=self.run_model_selection,\n",
    "                inputs=[\n",
    "                    df_state,\n",
    "                    target_var,\n",
    "                    model_type,\n",
    "                    X_train_state,\n",
    "                    y_train_state,\n",
    "                    X_val_state,\n",
    "                    y_val_state\n",
    "                ],\n",
    "                outputs=[\n",
    "                    training_curves_plot,\n",
    "                    metrics_plot,\n",
    "                    resource_plot,\n",
    "                    model_summary\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            train_model_button.click(\n",
    "                fn=self.handle_model_training,\n",
    "                inputs=[\n",
    "                    df_state,\n",
    "                    target_var,\n",
    "                    model_dropdown,\n",
    "                    X_train_state,\n",
    "                    y_train_state,\n",
    "                    X_val_state,\n",
    "                    y_val_state,\n",
    "                    model_type\n",
    "                ],\n",
    "                outputs=[\n",
    "                    optimization_plot,\n",
    "                    training_plot,\n",
    "                    training_summary\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            generate_report_button.click(\n",
    "                fn=self.handle_report_generation,\n",
    "                inputs=[],\n",
    "                outputs=[report_status]\n",
    "            )\n",
    "        \n",
    "        return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Launch the Application\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    app = AutomaticaUI()\n",
    "    demo = app.build_interface()\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
